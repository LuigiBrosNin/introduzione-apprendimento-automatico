# -*- coding: utf-8 -*-
"""2 - Computer Vision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n9Vmr6ZOzBgSxFvaua09k0ve1oozZ8wL
"""

!git clone https://github.com/samuelemarro/ml_intro

import cv2 as cv
import numpy as np
from ml_intro import samples, utils

"""# Istogrammi

A partire da un canale è possibile realizzare un istogramma dei valori più comuni:
"""

import matplotlib.pyplot as plt

# Sintassi: cv.calcHist(lista di immagini, lista di canali, maschera per calcolare solo in certi punti, numero di secchi, range dei valori)
histogram_red = cv.calcHist([samples.bird], [0], None, [256], [0, 256])
# Normalizziamo l'istogramma mettendo il picco a 1
histogram_red /= np.max(histogram_red)

utils.plot_image(samples.bird[:, :, 0], space='gray')
plt.plot(histogram_red, color='r')
plt.show()

"""Usando più canali si può avere una visione più ampia dell'immagine:"""

histogram_green = cv.calcHist([samples.bird], [1], None, [256], [0, 256])
histogram_green /= np.max(histogram_green)
histogram_blue = cv.calcHist([samples.bird], [2], None, [256], [0, 256])
histogram_blue /= np.max(histogram_blue)

utils.plot_image(samples.bird)
plt.plot(histogram_red, color='r')
plt.plot(histogram_green, color='g')
plt.plot(histogram_blue, color='b')
plt.legend(['R', 'G', 'B'])
plt.show()

"""RGB non ci dice però molte informazioni, proviamo con HSV:"""

bird_hsv = cv.cvtColor(samples.bird, cv.COLOR_RGB2HSV)

histogram_hue = cv.calcHist([bird_hsv], [0], None, [256], [0, 256])
histogram_hue = histogram_hue.astype(np.float32) / np.max(histogram_hue)
histogram_saturation = cv.calcHist([bird_hsv], [1], None, [256], [0, 256])
histogram_saturation /= np.max(histogram_saturation)
histogram_value = cv.calcHist([bird_hsv], [2], None, [256], [0, 256])
histogram_value /= np.max(histogram_value)

utils.plot_image(samples.bird)
plt.plot(histogram_hue, color='purple')
plt.plot(histogram_saturation, color='black')
plt.plot(histogram_value, color='orange')
plt.legend(['H', 'S', 'V'])
plt.show()

"""Calcolando gli istogrammi medi si possono ricavare statistiche utili:"""

utils.plot_images(samples.histogram.apples, columns=5)
utils.plot_images(samples.histogram.fish, columns=5)

apple_histogram_sum = np.zeros([256, 1], dtype=np.float32)
fish_histogram_sum = np.zeros([256, 1], dtype=np.float32)

for apple in samples.histogram.apples:
    # Usiamo il canale blu
    apple = cv.cvtColor(apple, cv.COLOR_RGB2HSV)
    apple_histogram = cv.calcHist(apple, [0], None, [256], [0, 256])
    apple_histogram /= np.max(apple_histogram)
    apple_histogram_sum += apple_histogram

for fish in samples.histogram.fish:
    # Usiamo il canale blu
    fish = cv.cvtColor(fish, cv.COLOR_RGB2HSV)
    fish_histogram = cv.calcHist(fish, [0], None, [256], [0, 256])
    fish_histogram /= np.max(fish_histogram)
    fish_histogram_sum += fish_histogram

apple_histogram_mean = apple_histogram_sum / 5
fish_histogram_mean = fish_histogram_sum / 5


plt.plot(apple_histogram_sum, color='red')
plt.plot(fish_histogram, color='blue')
plt.legend(['Apples', 'Fish'])
plt.show()

"""Possiamo quindi stabilire in maniera automatica se un'immagine è una mela o un pesce? Sì, confrontando i due istogrammi:"""

apple_test_hsv = cv.cvtColor(samples.histogram.apple_test, cv.COLOR_RGB2HSV)
apple_test_histogram = cv.calcHist([apple_test_hsv], [0], None, [256], [0, 256])
apple_test_histogram /= np.max(apple_test_histogram)

utils.plot_image(samples.histogram.apple_test)

plt.plot(apple_test_histogram, color='green')
plt.legend(['An apple?'])
plt.show()
plt.plot(apple_histogram_mean, color='red')
plt.plot(fish_histogram_mean, color='blue')
plt.legend(['Apples', 'Fish'])
plt.show()

"""Ci sono molti metodi per confrontare due istogrammi. Useremo la correlazione, che va da -1 (anticorrelati) a +1 (correlati):"""

print(apple_test_histogram.dtype)
print(apple_histogram_mean.dtype)
apple_apple_correlation = cv.compareHist(apple_test_histogram, apple_histogram_mean, cv.HISTCMP_CORREL)
apple_fish_correlation = cv.compareHist(apple_test_histogram, fish_histogram_mean, cv.HISTCMP_CORREL)

print('Correlazione con l\'istogramma delle mele:', apple_apple_correlation)
print('Correlazione con l\'istogramma dei pesci:', apple_fish_correlation)

"""Ovviamente non è sempre perfetto come metodo:"""

fish_incorrect_hsv = cv.cvtColor(samples.histogram.fish_incorrect, cv.COLOR_RGB2HSV)
fish_incorrect_histogram = cv.calcHist([fish_incorrect_hsv], [0], None, [256], [0, 256])
fish_incorrect_histogram /= np.max(fish_incorrect_histogram)

utils.plot_image(samples.histogram.fish_incorrect)

plt.plot(fish_incorrect_histogram, color='green')
plt.show()

fish_apple_correlation = cv.compareHist(fish_incorrect_histogram, apple_histogram_mean, cv.HISTCMP_CORREL)
fish_fish_correlation = cv.compareHist(fish_incorrect_histogram, fish_histogram_mean, cv.HISTCMP_CORREL)

print('Correlazione con l\'istogramma delle mele:', fish_apple_correlation)
print('Correlazione con l\'istogramma dei pesci:', fish_fish_correlation)

"""# Thresholding

La volta scorsa abbiamo guardato i valori dei pixel per determinare se erano pixel chiari (quindi sfondo) o scuri (quindi l'oggetto d'interesse). Questo è un esempio di _thresholding_. Il thresholding può essere applicato solo a immagini a un canale, quindi è importante scegliere quale canale prendere.

cv2 offre diverse opzioni di thresholding:
"""

def to_grayscale(image):
    return cv.cvtColor(image, cv.COLOR_RGB2GRAY)

# L'immagine è in bianco e nero, ma va comunque convertita in un solo canale
grayscale_gradient = to_grayscale(samples.special.bw_gradient)

# Sintassi: cv.threshold(immagine, valore di threshold, valore per indicare "true", tipo di thresholding)
# Restituisce il valore usato come threshold e l'immagine con il threshold applicato

_, binary_image = cv.threshold(grayscale_gradient, 127, 255, cv.THRESH_BINARY) # Se è minore di 127 diventa 0, altrimenti 255
_, binary_inv_image = cv.threshold(grayscale_gradient, 127, 255, cv.THRESH_BINARY_INV) # Se è minore di 127 diventa 255, altrimenti 0

utils.plot_images([
    grayscale_gradient,
    binary_image,
    binary_inv_image,
], titles=['original', 'binary', 'binary_inv'], columns=3, space='gray')

grayscale_bird = to_grayscale(samples.bird)

_, binary_image = cv.threshold(grayscale_bird, 127, 255, cv.THRESH_BINARY)
_, binary_inv_image = cv.threshold(grayscale_bird, 127, 255, cv.THRESH_BINARY_INV)

utils.plot_images([grayscale_bird, binary_image, binary_inv_image], titles=['original', 'binary', 'binary_inv'], columns=3, space='gray')

"""La maggior parte delle volte si usa `cv.THRESH_BINARY`.

Se non sappiamo il threshold da usare, si può usare la binarizzazione di Otsu:
"""

grayscale_apple = to_grayscale(samples.special.apple)

chosen_threshold, thresholded_apple = cv.threshold(grayscale_apple, 12345, 255, cv.THRESH_BINARY + cv.THRESH_OTSU) # Il parametro threshold (12345) viene ignorato

print('Chosen threshold:', chosen_threshold)
utils.plot_images([grayscale_apple, thresholded_apple], titles=['original', 'otsu\'s thresholding'], space='gray')

"""Curiosità tecnica: la binarizzazione di Otsu sceglie il threshold guardando l'istogramma e separandolo in due "distribuzioni"."""

histogram_apple = cv.calcHist([samples.special.apple], [0], None, [256], [0, 256])
histogram_apple /= np.max(histogram_apple)
plt.plot(histogram_apple, color='black')
plt.plot([chosen_threshold, chosen_threshold], [0, 1], color='red')
plt.show()

"""### Thresholding Adattivo

A volte un threshold è adeguato in certe zone ma non in altre: con l'adaptive thresholding, viene scelto un threshold locale in base ai pixel circostanti
"""

grayscale_sudoku = to_grayscale(samples.special.sudoku)
otsu_sudoku = cv.threshold(grayscale_sudoku, 12345, 255, cv.THRESH_OTSU)[1]

# Sintassi: cv.adaptiveThreshold(immagine, valore teorico massimo, tipo di scelta threshold, tipo di thresholding, dimensione dell'area di ricerca (dispari), costante che viene sottratta)
adaptive_mean_sudoku = cv.adaptiveThreshold(grayscale_sudoku, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 2)
adaptive_gaussian_sudoku = cv.adaptiveThreshold(grayscale_sudoku, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 11, 2)

utils.plot_images([grayscale_sudoku, otsu_sudoku, adaptive_mean_sudoku, adaptive_gaussian_sudoku], titles=['original', 'otsu', 'adaptive mean', 'adaptive gaussian'], space='gray')

"""Sfocare l'immagine può aiutare a "pulire" il risultato (anche con il thresholding normale):"""

grayscale_blur_sudoku = cv.medianBlur(grayscale_sudoku, 5)
otsu_blur_sudoku = cv.threshold(grayscale_blur_sudoku, 12345, 255, cv.THRESH_OTSU)[1]

# Sintassi: cv.adaptiveThreshold(immagine, valore teorico massimo, tipo di scelta threshold, tipo di thresholding, dimensione dell'area di ricerca (dispari), costante che viene sottratta)
adaptive_mean_blur_sudoku = cv.adaptiveThreshold(grayscale_blur_sudoku, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 2)
adaptive_gaussian_blur_sudoku = cv.adaptiveThreshold(grayscale_blur_sudoku, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 11, 2)

utils.plot_images(
    [grayscale_blur_sudoku, otsu_blur_sudoku, adaptive_mean_blur_sudoku, adaptive_gaussian_blur_sudoku],
    titles=['blurred', 'otsu (blurred)', 'adaptive mean (blurred)', 'adaptive gaussian (blurred)'],
    space='gray'
)

"""### Oltre il Grayscale

Il thresholding non deve essere per forza applicato a immagini grayscale:
"""

plane_red_channel = samples.plane[:, :, 0]

_, plane_red = cv.threshold(plane_red_channel, 205, 255, cv.THRESH_BINARY)

utils.plot_image(samples.plane)
utils.plot_image(plane_red, space='gray')

"""Nota che ha anche preso la nuvola perché il bianco ha un canale rosso molto alto.

### Combinare Thresholding

Il risultato di un'operazione di thresholding è un tensore, quindi si può tranquillamente applicare funzioni matematiche. `np.maximum` prende il massimo di due tensori (e quindi fa una sorta di OR logico), mentre `np.minimum` prende il minimo di due tensori (e quindi fa una sorta di AND logico). Fare `255 - image` è una sorta di NOT logico.
"""

_, red_threshold = cv.threshold(samples.plane[:, :, 0], 125, 255, cv.THRESH_BINARY)
_, green_threshold = cv.threshold(samples.plane[:, :, 1], 140, 255, cv.THRESH_BINARY)
_, blue_threshold= cv.threshold(samples.plane[:, :, 2], 75, 255, cv.THRESH_BINARY)

green_or_blue = np.maximum(green_threshold, blue_threshold).astype(np.uint8)
only_red = np.minimum(red_threshold, 255 - green_or_blue).astype(np.uint8)

utils.plot_images([
    red_threshold,
    green_threshold,
    blue_threshold,
    green_or_blue,
    only_red
], titles=['red', 'green', 'blue', 'green or blue', 'only red'], columns=3, space='gray')

"""In alternativa, è possibile convertire le immagini in tensori booleani (es. facendo `np.equal(image, 255)`) e poi lavorarci sopra con operatori booleani bitwise (`&`, `|`, `~`).

## Masking
"""

def apply_mask(image, mask):
    return image * (np.equal(mask, 255)).astype(np.uint8).reshape(mask.shape + (1,))

utils.plot_image(only_red, space='gray')
utils.plot_images([samples.plane, apply_mask(samples.plane, only_red)], titles=['original', 'masked'])

"""### Erosione e Dilatazione
cv2 ha anche dei metodi per erodere e dilatare le immagini:
"""

def erode(image, erosion_size):
    erosion_shape = cv.MORPH_ELLIPSE
    erosion_element = cv.getStructuringElement(erosion_shape, (2 * erosion_size + 1, 2 * erosion_size + 1),
                                        (erosion_size, erosion_size))
    return cv.erode(image, erosion_element)

def dilate(image, dilation_size):
    dilation_shape = cv.MORPH_ELLIPSE

    dilation_element = cv.getStructuringElement(dilation_shape, (2 * dilation_size + 1, 2 * dilation_size + 1), (dilation_size, dilation_size))
    return cv.dilate(image, dilation_element)

text_image = to_grayscale(samples.special.boomer)
eroded = erode(text_image, 2)
dilated = dilate(text_image, 2)

utils.plot_images([text_image, eroded, dilated], titles=['original', 'eroded', 'dilated'], space='gray', columns=3)

"""Erosione e dilatazione possono essere usati per espandere o restringere le mask.

## Esercizio: Thresholding

Scrivete una funzione che selezioni i pappagalli da questa immagine:

<img width="350" src= "https://raw.githubusercontent.com/samuelemarro/ml_intro/master/img/special/parrots.png"/>

Questa è la maschera di "soluzione" (fatta manualmente con Photoshop):

<img width="350" src= "https://raw.githubusercontent.com/samuelemarro/ml_intro/master/img/special/parrots_mask.png"/>

Il punteggio è calcolato come _area dell'intersezione tra le maschere / area dell'unione delle maschere_.

<details>
<summary>Suggerimento</summary>
A volte è più facile selezionare tutto ciò che non è interessante invece di tutto ciò che è interessante.
</details>
"""

def find_mask(image):
    # Messa come esempio
    hsv_image = cv.cvtColor(image, cv.COLOR_RGB2HSV)
    utils.plot_image(hsv_image[:, :, 0])
    _, red_threshold = cv.threshold(hsv_image[:, :, 0], 75, 255, cv.THRESH_BINARY_INV)
    _, nonred_threshold = cv.threshold(hsv_image[:, :, 0], 75, 255, cv.THRESH_BINARY)
    return red_threshold
    return red_threshold

def compute_difference_image(current_mask, target_mask):
    difference_image = np.zeros(current_mask.shape + (3,), dtype=np.uint8)
    boolean_current_mask = np.equal(current_mask, 255)
    boolean_target_mask = np.equal(target_mask, 255)
    # La parte corretta della maschera è bianca
    difference_image[boolean_current_mask & boolean_target_mask] = (255, 255, 255)
    # La parte che dovrebbe essere della maschera ma non lo è viene colorata di rosso
    difference_image[~boolean_current_mask & boolean_target_mask] = (255, 0, 0)
    # La parte che non dovrebbe essere della maschera ma lo è viene colorata di blu
    difference_image[boolean_current_mask & ~boolean_target_mask] = (0, 0, 255)

    return difference_image

def intersection_over_union(current_mask, target_mask):
    boolean_current_mask = np.equal(current_mask, 255)
    boolean_target_mask = np.equal(target_mask, 255)

    intersection = np.count_nonzero(boolean_current_mask & boolean_target_mask)
    union = np.count_nonzero(boolean_current_mask | boolean_target_mask)
    return intersection / union

original_parrots = samples.special.parrots
mask_parrots = find_mask(np.copy(original_parrots))
target_parrots = to_grayscale(samples.special.parrots_mask)
difference_image = compute_difference_image(mask_parrots, target_parrots)

utils.plot_images([mask_parrots, target_parrots], space='gray', titles=['current mask', 'target mask'])

utils.plot_images(
    [original_parrots, apply_mask(original_parrots, mask_parrots), apply_mask(original_parrots, target_parrots), difference_image],
    titles=['original', 'masked', 'target', 'difference'],
    columns=4
)
print('Punteggio: {:.2f}%'.format(intersection_over_union(mask_parrots, target_parrots) * 100))

"""## Rilevamento contorni
Una volta che abbiamo una maschera possiamo anche tracciare i contorni degli oggetti:
"""

gray_birds = to_grayscale(samples.special.birds)
_, birds_mask = cv.threshold(gray_birds, 64, 255, cv.THRESH_BINARY_INV)
# Sintassi: cv.findContours(immagine, tipo di retrieval, tipo di approssimazione)
# Restituisce la lista dei contorni e la gerarchia tra contorni (chi contiene quali contorni)
# Tipi comuni di retrieval:
# - RETR_EXTERNAL (solo i contorni esterni)
# - RETR_LIST (tutti i contorni)
# - RETR_CCOMP (contorni esterni + contorni dei buchi interni)
contours, _ = cv.findContours(birds_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)

# Sintassi: cv.drawContours(immagine di partenza, contorni, indice del contorno o -1 per tutti gli indici, colore, spessore oppure cv.FILLED per riempire)
image_with_contours = cv.drawContours(samples.special.birds.copy(), contours, -1, (255, 0, 0), 3)

utils.plot_images([samples.special.birds, birds_mask, image_with_contours], titles=['original', 'mask', 'contours'], columns=3)

print('Numero di rondini:', len(contours))

"""In alternativa si può anche disegnare una bounding box:"""

image_with_rects = samples.special.birds.copy()
for contour in contours:
    x, y, width, height = cv.boundingRect(contour)
    image_with_rects = cv.rectangle(image_with_rects.copy(), (x, y), (x+width, y+height), (255, 0, 0), 3)

utils.plot_image(image_with_rects)

"""## Watershed

Spesso non si riesce a ottenere maschere esatte con il thresholding, oppure non si riesce a separare due oggetti diversi. Per fare maschere più avanzate si utilizza l'algoritmo Watershed.

Watershed ha bisogno di tre informazioni:
- Quali pixel appartengono sicuramente all'oggetto di interesse
- Quali pixel appartengono sicuramente allo sfondo
- Quali pixel appartengono a oggetti diversi tra di loro
"""

utils.plot_image(samples.special.coins)

"""Prima di tutto separiamo l'oggetto di interesse dallo sfondo:"""

coins_gray = cv.cvtColor(samples.special.coins, cv.COLOR_RGB2GRAY)
utils.plot_image(coins_gray, space='gray')

_, thresholded_coins = cv.threshold(coins_gray, 12345, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)
utils.plot_image(thresholded_coins, space='gray')

"""Se erodiamo la maschera otteniamo pixel che sono sicuramente monete, mentre se dilatiamo la maschera e la invertiamo otteniamo pixel che sono sicuramente sfondo:"""

foreground = erode(thresholded_coins, 10)
background = 255 - dilate(thresholded_coins, 5)

utils.plot_image(samples.special.coins)
utils.plot_images([foreground, background], titles=['foreground', 'background'], space='gray')

"""Per ottenere monete separate, erodiamo ancora di più:"""

definitely_coins = erode(foreground, 7)

utils.plot_image(definitely_coins, space='gray')

"""Possiamo ora tracciare i contorni e "riempirli" con numeri progressivi a partire da 2 (1 lo usiamo per lo sfondo):"""

markers = np.zeros([samples.special.coins.shape[0], samples.special.coins.shape[1]])
# Riempi di 1 lo sfondo
markers += (background / 255) * 1

contours, _ = cv.findContours(definitely_coins, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
drawn = np.zeros_like(samples.special.coins)
cv.drawContours(drawn, contours, -1, (255, 255, 255), 1)
utils.plot_image(drawn, title='contours')

i = 2
for contour in contours:
    markers = cv.drawContours(markers.copy(), [contour], -1, (i), cv.FILLED)
    i += 1

# Immagine a falsi colori
utils.plot_image(markers, title='markers')

"""Infine, lanciamo l'algoritmo watershed:"""

found_markers = cv.watershed(samples.special.coins, markers.astype(np.int32))
utils.plot_image(found_markers)

# Il contorno ha valore -1
coins_with_contours = np.copy(samples.special.coins)
coins_with_contours[found_markers == -1] = [255, 0, 0]
utils.plot_image(coins_with_contours)

"""# Elaborazione Video
Tutte le operazioni di cui abbiamo parlato funzionano anche con i video:
"""

# Tutto questo blocco di codice serve solo per lavorare con la webcam in Colab, non serve saperlo

# function to convert the JavaScript object into an OpenCV image
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import numpy as np
import PIL
import io
import html
import time

def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv.imdecode(jpg_as_np, flags=1)
  img = cv.cvtColor(img, cv.COLOR_BGR2RGB)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes

# JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;
    
    var pendingResolve = null;
    var shutdown = false;
    
    function removeDom() {
       stream.getVideoTracks()[0].stop();
       video.remove();
       div.remove();
       video = null;
       div = null;
       stream = null;
       imgElement = null;
       captureCanvas = null;
       labelElement = null;
    }
    
    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }
    
    async function createDom() {
      if (div !== null) {
        return stream;
      }

      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = '100%';
      div.style.maxWidth = '600px';
      document.body.appendChild(div);
      
      const modelOut = document.createElement('div');
      modelOut.innerHTML = "<span>Status:</span>";
      labelElement = document.createElement('span');
      labelElement.innerText = 'No data';
      labelElement.style.fontWeight = 'bold';
      modelOut.appendChild(labelElement);
      div.appendChild(modelOut);
           
      video = document.createElement('video');
      video.style.display = 'block';
      video.width = div.clientWidth - 6;
      video.setAttribute('playsinline', '');
      video.onclick = () => { shutdown = true; };
      stream = await navigator.mediaDevices.getUserMedia(
          {video: { facingMode: "environment"}});
      div.appendChild(video);

      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute';
      imgElement.style.zIndex = 1;
      imgElement.onclick = () => { shutdown = true; };
      div.appendChild(imgElement);
      
      const instruction = document.createElement('div');
      instruction.innerHTML = 
          '<span style="color: red; font-weight: bold;">' +
          'Clicca qui o sul video per fermare</span>';
      div.appendChild(instruction);
      instruction.onclick = () => { shutdown = true; };
      
      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; //video.videoWidth;
      captureCanvas.height = 480; //video.videoHeight;
      window.requestAnimationFrame(onAnimationFrame);
      
      return stream;
    }
    async function stream_frame(label, imgData) {
      if (shutdown) {
        removeDom();
        shutdown = false;
        return '';
      }

      var preCreate = Date.now();
      stream = await createDom();
      
      var preShow = Date.now();
      if (label != "") {
        labelElement.innerHTML = label;
      }
            
      if (imgData != "") {
        var videoRect = video.getClientRects()[0];
        imgElement.style.top = videoRect.top + "px";
        imgElement.style.left = (videoRect.left + videoRect.width) + "px";
        imgElement.style.width = videoRect.width + "px";
        imgElement.style.height = videoRect.height + "px";
        imgElement.src = imgData;
      }
      
      var preCapture = Date.now();
      var result = await new Promise(function(resolve, reject) {
        pendingResolve = resolve;
      });
      shutdown = false;
      
      return {'create': preShow - preCreate, 
              'show': preCapture - preShow, 
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)
  
def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
  return data

def apply_transformation(func):
    video_stream()
    # label for video
    label_html = 'Capturing...'
    # initialze bounding box to empty
    bbox = ''
    count = 0 
    while True:
        js_reply = video_frame(label_html, bbox)
        if not js_reply:
            break

        # convert JS response to OpenCV Image
        img = js_to_image(js_reply["img"])

        # create transparent overlay for bounding box
        bbox_array = np.zeros([480,640,4], dtype=np.uint8)

        # grayscale image for face detection
        transformed_image = func(img)
        bbox_array[:, :, :3] = transformed_image


        bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255
        bbox_bytes = bbox_to_bytes(bbox_array)
        bbox = bbox_bytes

def red_tint(image):
    image = np.copy(image).astype(np.int16)
    image[:, :, 0] += 40
    image[:, :, 1] -= 40
    image[:, :, 2] -= 40
    return np.clip(image, 0, 255).astype(np.uint8)

def red_threshold(image):
    _, red_thresholded_image = cv.threshold(image[:, :, 0], 200, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)
    _, green_thresholded_image = cv.threshold(image[:, :, 1], 100, 255, cv.THRESH_BINARY)
    _, blue_thresholded_image = cv.threshold(image[:, :, 2], 100, 255, cv.THRESH_BINARY)
    only_red = np.minimum(red_thresholded_image, 255 - green_thresholded_image, 255 - blue_thresholded_image)
    modified_image = np.copy(image)
    modified_image[:, :, 0] = only_red
    modified_image[:, :, 1] = only_red
    modified_image[:, :, 2] = only_red
    return modified_image

def red_bounding_box(image):
    _, red_thresholded_image = cv.threshold(image[:, :, 0], 200, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)
    _, green_thresholded_image = cv.threshold(image[:, :, 1], 100, 255, cv.THRESH_BINARY)
    _, blue_thresholded_image = cv.threshold(image[:, :, 2], 100, 255, cv.THRESH_BINARY)
    only_red = np.minimum(red_thresholded_image, 255 - green_thresholded_image, 255 - blue_thresholded_image)
    contours, _ = cv.findContours(only_red, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
    # Ordina dal più grande al più piccolo
    contours = sorted(contours, key=lambda x: cv.contourArea(x), reverse=True)
    # Disegna un rettangolo intorno all'oggetto più grande
    if len(contours) == 0:
        return image
    x, y, width, height = cv.boundingRect(contours[0])
    image_with_rects = cv.rectangle(image.copy(), (x, y), (x+width, y+height), (0, 0, 255), 3)
    return image_with_rects

#apply_transformation(red_tint)
#apply_transformation(red_threshold)
#apply_transformation(red_bounding_box)

"""## Esercizio: Occhiali da Sole

Scrivete una funzione che disegna degli occhiali da sole addosso alla persona di una foto. Opzionalmente potete anche utilizzare la webcam.

<details>
<summary>Suggerimento</summary>
Un occhio non è altro che una massa bianca circondata da una massa colorata (il volto). Una volta che sai dove si trovano i due occhi, è facile determinare il centro degli occhiali
</details>
"""

def draw_sunglasses(image, x, y, width, height):
    sunglasses_alpha = cv.resize(np.copy(samples.special.sunglasses_alpha), [width, height])
    actual_width = max(min(x + width, image.shape[1]) - x, 0)
    actual_height = max(min(y + height, image.shape[0]) - y, 0)
    if actual_width == 0 or actual_height == 0:
        return image
    sunglasses_alpha = sunglasses_alpha[:actual_height, :actual_width, :]
    image = np.copy(image)
    mask = sunglasses_alpha[:, :, 3] / 255
    mask = mask.reshape([sunglasses_alpha.shape[0], sunglasses_alpha.shape[1], 1])
    image[y:y + actual_height, x:x + actual_width, :3] = sunglasses_alpha[:, :, :3] * mask + image[y:y+actual_height, x:x+actual_width, :3] * (1 - mask)

    return image

def sunglasses_on_face(image):
    return image

utils.plot_image(sunglasses_on_face(samples.bird))
# apply_transformation(sunglasses_on_face)

"""# Esercizio Extra: Conteggio

Contate quante persone ci sono nella seguente immagine:
<img width="800px" src="https://github.com/samuelemarro/ml_intro/blob/master/img/special/people.png?raw=true" />
"""

utils.plot_image(samples.special.people)

"""# Cose non Viste
OpenCV ha molti strumenti e i tutorial della documentazione sono molto buoni. Alcuni consigliati:
- [Creare immagini HDR](https://docs.opencv.org/4.x/d3/db7/tutorial_hdr_imaging.html)
- [Stimare la posizione 3D di un oggetto (utile per la realtà aumentata)](https://docs.opencv.org/4.x/d7/d53/tutorial_py_pose.html)
- [Inpainting (il "Riempimento Intelligente" di Photoshop)](https://docs.opencv.org/4.x/df/d3d/tutorial_py_inpainting.html)

Altri strumenti utili non-neurali:
- [AprilTag](https://april.eecs.umich.edu/software/apriltag) è un sistema per tracciare con alta qualità qualunque oggetto usando cubi di cartone e una webcam. Utile per applicazioni in robotica, tracciamento del corpo umano e anche VR
- [Scikit-image](https://scikit-image.org/) contiene alcune tecniche avanzate per tracciare contorni, segmentare immagini e altri utilizzi di Computer Vision
"""